期望的步骤：

得到一个数据集：点集：idx:weight, 边集：idx1, idx2, e_weight。这样组成的一个大数据集和若干小数据集，可以有多组数据

这样怎么从表示点的idx转化为可以计算的vector呢？查阅资料，vf2没有转化，直接使用原数据计算。GCN，SAGE，等使用feature维，并与隐藏层做卷积，构成了点的embedding.可是，我们的数据不存在feature这个域

VF2怎么嵌入到强化学习的代码里

## 从idx转化为vector:

[diease_gene_prioritization_GCN]使用tensorflow，使用了feature，使用了大量邻接矩阵，但是看不懂怎么使用的邻接矩阵

GCN原文中提到，如果没有feature，可以使用I替代

基于频率的编码：$\sum$维的向量，第i位是$e_v^{(0)}[i]=F(l_i)/|V|$如果v和第i个lable有关，否则$e_v^{(0)}[i]=1.0$，F(li)是label L出现的频率，

基于嵌入的编码：构建label增强图，把label作为新的节点添加到图里。使用图嵌入算法预训练node embedding。对于nodev,$e_v^{(0)}=\sum_{l \in L(v)}e'(l)$其中e'(l)是nodel的label l对应的预训练embedding



action_dim：由于是直接看当前action和state的哪一行距离最近，所以应该是一行的长度=2*V_num



有个问题，state中应该是容纳大图的矩阵还是子图的矩阵呢？

# 目前存在的问题

### 每次都是说大图和小图没有匹配上，应该忽略这条信息继续进行吗？

当前的策略是：要求子图的该点的邻居数 = 大图的该点的邻居数。因此生成子图时边应该适当稠密？

label为什么是空？已解决

### 每次只匹配了一两个点是正常的吗？如何解决

每次都会随机选择一个大图中的点，如果没有匹配就不输出，所以不输出也是在不断的判断是否有合适的点，这里需要一个早停策略，避免空转

2000次只有一两次能找到匹配的点，扩大搜索次数管用吗。或者应该看一下输出的向量是什么，为什么其他的轮次都没有被选择？

几乎都是因为不满足查找规则

连续的idstance_action几乎大小相差无几，难道是每次输出的都一致？可能是action变化太小了，输入同样的state就会输出几乎同样的action，是否应该加大random的力度？可是random的力度太大又会导致训练一直不收敛……

可能是添加一个点对于action的改变太小了？？？？ 目前看成功跳转到新的点的action的改变都比较大拍查了一下感觉action的变化还挺大的呀？？它只会选择其中的一部分点(例如20个点只选了2,5,9,0)难道是点的表示有问题？主图的表示怎么如此稀疏？？？大部分都是零！--不是的，不过是第一个图比较稀疏

在不稀疏的图里，大图依然只有较少的点被选中，可能是action太偏向于0 的缘故？

增大随机噪音之后虽然还有很大可能选择某个点，但是其他点被选择的可能也增大了

### 为什么子图被选择的都只是那几个点？？？

进行了去重操作，但是依然没有完美匹配的案例。匹配规则有没有特别的，把他们取了下来？

神经网络给出的结果就是错的，所以无法生成完全一致的图

现在增大了大图的边密度，按道理只要边密度够大，当不考虑label就能构成任意子图

### 训练的速度非常慢

是否删除一些遍历操作？？？当现有的数据终于填满storage的时候就会每过500iteration就train一次显得后面训练的很慢，总体没有错误

### 没有成功从断点继续训练？

现在似乎是遍历了一次所有图就开始反复训练了？也没有测试

测试的时候要么选择一对点，要么根本不选择，为什么呢。因为所有输出的action都是-1，模型到底被训练了什么啊

在测试的时候一旦state不能动，action必然不能动，是不是也应该改给予一定的探索限度？

如果加上了噪音那怎么不说这个就是被遍历出来的呢，20*100 = 2000啊，完全能遍历出来了

目前看来由state变化出的action变化太小了？？

是不是因为开始的state太过稀疏（一开始只有一个点！）没有考虑子图？生成新的state吧

怎么设计state的损失函数呢？让state和什么尽可能接近呢？或者能不能让其他的损失函数来更新state网络的参数呢？可以的哦，就是把处理state的代码直接放在critic里面？但是计算action的呢？放在action里面然后critic不考虑？

现在state对action的改变还是很小，如何让网络注意到这个微小的变化呢。现有的工作是怎么突出微小的变化呢，NLP

难道是学习率太高了？？？降低了学习率之后不同图的表示不一样了，但是同一图不同state还是一样的

增加正样本的比例？好像增大了之后action趋近于1更快了
noise太大了？减小到了0.1

能不能设计一个loss增大输入不同action的state的差异呢？**TODO**

目前把action输出的取值范围调到了0-1，还是没有摆脱直接收敛到两端啊

学姐说的只输入主图和主图的邻居还没有尝试过

![1644403477193](C:\Users\litia\AppData\Roaming\Typora\typora-user-images\1644403477193.png)

### 目前存在的问题：

有时存在跳不出循环的情况（前几个都能选对，但是不能选全）

错选：对应的点选错，可能是特征真的像，也可能是判断条件不对？

全选对的概率还是有点低

~~是不是应该去掉noise再试一次~~

几乎下一步都能正确选出来，但是下下一步就有点难了

这里猜测是第一步和后面的步骤难度不同，第一步oristate只给出了一个点对应的状态和邻居，是否应该所有的步骤都只给一个点的状态，而不是累计状态呢？目前还没看出有明显的好处，但是坏处似乎有点明显，就是它似乎局限于当前点给出的邻居了，如果当前点的邻居没戏，那模型就失效了？（当前点如果是对的，它一定有个邻居有戏？不是的，子图可能不是联通的）

先把这次的跑完，然后存好模型，再调整

#### 给的state里没有关于边的label，但是vf2的判断却要求边label一致？



#### 是不是vf2给的条件太严格了呢？

- 子图的前驱节点数应该小于等于大图前驱节点数
- 大图的前驱节点如果也被选择了，那么它应该和小图的前驱节点有对应，且与当前点链接的边也对应
- v1的邻居数不能大于大于大图点v2的邻居数
- (子图中所有点，除了result的邻居和v1的前驱)，剩下的点数   不能大于   (大图所有点-result对应的邻居和v2的前驱)   的点数

#### fre矩阵的表示是否多此一举？

查阅相关论文，属于独热编码的改进，其实用在edge-label中会更好



#### 查阅autoRL论文



#### noise是否应该继续存在

noise在训练的时候是为了增大探索程度，要不随着训练次数的增大减小noise？考虑把未能选择成功的点也加入当前的state里，但是给出的reward是负值，改变了state但是不应该改变最终判断输赢的那个吧，这样的话新的state就会出现新的可选项，最终提前出线全图？是否考虑修改main_state为只有当前的数据？一个当前状态state，一个表示可以选择的主图状态，一个表示子图，当前state只包含现在选择的点，以及过去成功选择点的对角线？

还有个问题，大图所选的点是通过state表示了，那对应哪个小图点却完全没有在里面表示出来，要不sub_state也修改？做减法，每选中一个点就删掉它的行。**sub_state已解决**。事实上，sub-state减去已经选中的点-state只保存当前的点都不能使模型表现更好，反而难以收敛



#### 可以调节的内容：

- 超参数
- NN架构
- state设计，reward设计

#### action为什么不能直接是行：大图选哪个点，列：小图选哪个点？

震荡的原因是完成的reward和未完成的reward差别太大了，方差过大

老师提出的思路：不要label...,同时输出多个action，大图再大点，一个大图多个子图，

这里是想用新的embedding来替换原来的卷积操作，原来的卷积是为了生成action，也就是找到大图和小图特征一致的点，（一致？还是允许更多？首先GCN肯定得卷2-3次，把邻居的信息编码进来，那么该怎么计算两者相似度呢？如果是矩阵相乘，n1\*channnel\*channel*n2，如果相似会有什么特征呢？越相似与小图的平方越接近。根据这个特性再sigmoid一下？就得到了相似度矩阵，然后就能输出n个点，这n个点还的要求是连在一起的

那么现有的算法怎么设计的相似度矩阵呢？人家设计的计算注意力思路跟我差不多，实现更成熟

那么gnn的设计对吗？基本还可以，那么问题来了，如果直接使用相似度矩阵来计算，还跟强化学习有关系吗？希望有关系，所以要使用state。

那么只使用gcn对图的embedding，然后采用强化学习。强化学习的目的是减小搜索空间并避免回溯。比如已知n步邻居信息，确定下一个要选的点，如果选的点连约束都满足不了reward=-1，如果满足了约束但不能构成结果，reward=0，如果选错了能否回溯呢？事实上应该允许回溯，也就是当reward不为1时，把上一个结果从state删掉or直接随机一个点重新开始？这里也就是通过强化学习给大图排序，暂时对每个子图点都是暴力搜索

那么按照这个思路，action应该是使用当前选择的点-点的k邻居构成的embedding与子图的embedding做相似度运算，找到相似度最大的点。如此一来是可以根据NTN输出点坐标的！

critic的本质是通过神经网络对当前state和state下的action做评估，评估需要尽可能接近reward。那么是否评估还需要参考sub_graph？action可以相当于一个attention吗？最好是state*action = 子图的评估，再将这个结果与sub_state做减法？

#### 老师提出的可以修改的点：

一个大图多个小图，大图再大一点。state的设计以后可以同时算3个选距离最近的。可以直接输出index，可以直接输出多个点，不要一次输出一个

#### 思考一下能不能回溯的问题：

应该支持回溯，例如扫描大图的点，如果发现这样扫描不可能得到结果，那么应该再也不考虑这个点并且回溯。那就是说把大图对应的这个小图加入self.result，回溯的话应该给惩罚！

如果小图中的点也被选择过了呢？那么替换小图点的选择为新的点，并且扣分

这里既然给出了大图vs小图的相似度，为什么不直接判断两者正确不，不正确就惩罚，正确就奖励？

还有一个重新开始问题，重新开始的话继续走之前的老路怎么办？可是已经随机选择开始的点了啊？？那就是要最大化类间距离了

在计算critic的时候，给一个att_sub和sub_state，如何计算两者的相似度？？adj不能带label

#### 将输入用开源图包例如dgl和networkx表示

如果只是为了实现gcn，可能没太大必要？我目前是把标准gcn换成了乘以更小倍数

#### 为什么输出都是0.001?

因为在1000这个维度矩阵相乘的结果都非常近似，避免结果近似的办法应该是？

假设1：是输入的差别太小：试过了不太靠谱

假设2：是计算架构的问题，gcn会让竖列趋向于相差无几，那么减少gcn层数如何？原来主要问题是要把过小的数或者过大的数放缩到1附近。现在看来gcn是能输出合理的值了，现在就是NTN的问题

#### 为什么训练不收敛：

- 大图搞一个随即遮盖？大图小图相似的关键是两点邻居结构是否一样，邻居边label，邻居点label等
- 过去n次已经选过的点不可以选
- 一旦选择正确就反馈，不应该隔一段时间才反馈

下图为刚改好GCN的训练结果，梯度消失？

2022-03-07-16-30-07\

![image-20220310154101839](C:\Users\rz\AppData\Roaming\Typora\typora-user-images\image-20220310154101839.png)

./2022-03-10-16-29-51/要求和过去10次的不一样，并不work，而且没有上一般的总体reward好

![image-20220315161544477](C:\Users\rz\AppData\Roaming\Typora\typora-user-images\image-20220315161544477.png)

接下来考虑先预训练一个GCN，根据图抠出k临近子图，然后计算两个大小不一样图同样的embedding，要求如果两个图有相似的部分，那么embedding也是相近的，如果两个图不相近，那么embedding距离远

回归到上一个选择策略--没有10个间隔期，可以回溯等……

GCN：输入是一个不定大小的图，输出是按照子图同构规范的embeddding，输入k跳子图，要和小图的k跳图的每一行做相似度计算？这样的话可以输出k跳子图和query的k跳子图的相似度矩阵。输入不仅有k跳图，还要有边属性图，还要有点属性图。点属性可以是一个n向量，每一位上对应点的label，当然点的label可以有多个，所以是n*k，一个点k个属性。那么对于k跳图应该按照包含关系训练，对于边属性图应该普通训练，点属性图普通训练。那么为什么非要用GCN的格式呢？我们的目标是输出相似度，三种输入采用三种方式得到相似度，然后三个逐点相乘

action是一个长度为state输入图点数量的概率值。这样就可以确定大图要选择的点是谁？

state是上次选择点的k跳邻居构成的矩阵（层序遍历）记录这个子图与原始大图的对应关系，然后

critic的本质是自动计算的reward，也就是预测的收益，那么已知action中最大的几个点对，把这几个点对抽取出来，得到大图子图和小图子图，大图越能覆盖i子图越正确。

这里需要格外设计的就是覆盖算子。输入不等大方阵a和b，输出把一个n*n的图映射为[t]的向量，这种算子一定是单向的，即A（a,b） ≠ A（b,a)适合减法配合RELU，且减法出来的值越大越好，a-b越大说明a越覆盖b，那么我们期望选择的就是最不覆盖的那个覆盖点对。神经网络就是在提取特征，而处理邻接矩阵最希望的就是保持



先用一重邻居做。

一个可变大小的state和一个固定大小的state，LGCN可以尝试组装某个节点的固定数量邻居为自己的特征，并对得到的特征排序选择前n个作为输入。**注意，可以不考虑如何找到第一个点！！** 先找到第一个点，然后根据第一个点的邻居逐渐找后面的，能不能扩充子图到大图的维度，这样两者方便比较每一行是否一致，行向量一致，那么子图需要转置，然后旋转90度，这样本来越近似的，就越垂直，值就越小，选最小的值！！！特征排序还需要控制方法，选择包含性好的

### :TODO 

- ~~生成数据集，子图用DFS~~
- 阅读LGCN和~~神经子图匹配~~两篇论文
- 实现
- 关于order emdedding和强化学习如何结合的思考

## order embedding的实现：

训练情况：

分辨负样本表现不好，认为可以扩展数据集

![image-20220324224250345](C:\Users\rz\AppData\Roaming\Typora\typora-user-images\image-20220324224250345.png)

![image-20220411162900872](C:\Users\rz\AppData\Roaming\Typora\typora-user-images\image-20220411162900872.png) 准确率0.85



### 如何结合强化学习和order呢？

状态：图的表示：node_fature+adj，开始为随机点或者生物中的碳原子
action:开始点为现有图中任意一点，结束点为现有图+候选点-开始点（我们的可以是开始为已选点的任何一点，结束点为已选点的邻居-已选点）
policy（actor）：图生成器，输入图的表示和候选集，输出可能action的概率，使用reawrd反馈训练
reward：一部分来自GNN关于第i类输出的概率（即期望生成的图都是第i类）另一部分是根据rule来确定图是否valid

order的输出是什么？输入一个图所有点的feature，可以通过简单的GCN转化过来。order主要包含两部分一部分是embedding将node矩阵转化为一个node_featurex?长度的向量表示整个图，主要通过pyg_nn.global_add_pool进行，一个图中所有点的特征逐列相加（这样加的维度是node_feature*n_layer+1不知道为啥），再经过全连接变为需要的维度

那也就是将图生成器的位置替换为order，期望如何长成子图同构

那么原本的order就需要修改，需要输入现有点构成的矩阵，期望加的点的特征（能否根据一个点的邻居生成它的特征呢）。query中已选择点构成的矩阵和现有点的邻居构成的矩阵，那么第一个选的是小图的点，第二个选的是大图的点。至少不需要生成node_feature的向量，保留node* node_feature的结构，

大图已选点拼接小图候选点，使用order target为小图已选点+小图候选点，要求大拼小是小拼小的子图！这样再MLP+softmax就可以？？

选好了开始点后，将小图构建好，将现有大图和大图候选点拼接起来，使用order（至少不需要生成node_feature的向量，保留node* node_feature的结构），再经过一个MLP+softmax得到第二个点

如何反馈？

695,1 - after_emb695,64, all_emb695,1,64

1. 输入我生成的图数据，每次输出已选点和邻居的图数据，x,edge_index,batch? 有找邻居的能力

![image-20220413105137621](C:\Users\rz\AppData\Roaming\Typora\typora-user-images\image-20220413105137621.png)

2. ~~完成了actor的流程，但是送入actor的图数据nx如何转化为tensor和batch呢，可能需要order-embed帮忙；另外env文件有些不匹配的地方了；mask也没有实现~~
2. ~~完成了step的编写，正在写push memory~~
2. ~~没写完push memory，现在在写critic考虑和reward2合并，也就是删掉reward2，搞成critic. 另外critic的loss计算是不是太不连续了；；；另外order-embedding训练一直中途停止，为什么呢~~
2. ~~框架写完了，但是一个order model用三遍不太好，应该在后期考虑复制出来，训练新的ckpt~~
2. ~~network没有边权，现在好了~~
2. critic部分需要仿照order model' critirion 添加预测错误时的最小loss保证.目前order-model明显输出的emb太大了，所以算出的q值和reward匹配不上，直接使用负值将会导致没有办法收敛，需要将order-model的输出和1-0范围绑定，所以要先改order-model

### order embedding的修正和可视化

order只关注点的label不关注边的label需要注意

#### BN不收敛？换个激活函数？

首先尝试只在最后一层激活函数变为sigmoid，其他为relu,发现准确率是可以稳步上升的。但是embedding的输出是行向量几乎一致，列向量稍有不同。但是行向量是表示一个独特的点，点之间越是相似，越偏离想要的结果。

是否尝试将总模型的pred函数修改为用神经网络？？改不了，那就警惕点于点之间的特征混淆；观察到点越少，点之间的差异性越大，点越多，点之间越相似

SAGE是将邻居信息采样和聚合，是不是因为我生成的数据集邻居都大概一致，所以才生成的特征相似？

【现在转为layer=4】

修改后各个数据似乎恢复了合理的区间，但是正例太少，不好训练啊：下面给出了重要几个数据的值，基本正常，能训练：

> 检查计算出的数值是否符合要求: reward: ,  tensor([[-1.],
>         [-1.],
>        。。。
>         [-1.],
>         [-1.],
>         [-1.],
>         [-1.]], device='cuda:2') target_q:  tensor([[-0.0153],
>         [-0.0103],
>         [-0.0133],
>         [-0.0100],
>         [-0.0153],
>         [-0.0119],
>         [-0.0117],
>         [-0.0100],
>         [-0.0146],
>         [-0.0221],
>         [-0.0195],
>         [-0.0221],
>         [-0.0100],
>         [-0.0292],
>         [-0.0104],
>         [-0.0250],
>         [-0.0191],
>         [-0.0164],
>         [-0.0198],
>         [-0.0182],
>         [-0.0110],
>         [-0.0314],
>         [-0.0144],
>         [-0.0124],
>         [-0.0175],
>         [-0.0230],
>         [-0.0162],
>         [-0.0330],
>         [-0.0151],
>         [-0.0158],
>         [-0.0305],
>         [-0.0194],
>         [-0.0144],
>         [-0.0183],
>         [-0.0104],
>         [-0.0111],
>         [-0.0277],
>         [-0.0228],
>         [-0.0136],
>         [-0.0273],
>         [-0.0197],
>         [-0.0101],
>         [-0.0133],
>         [-0.0120],
>         [-0.0101],
>         [-0.0150],
>         [-0.0256],
>         [-0.0101],
>         [-0.0150],
>         [-0.0225],
>         [-0.0105],
>         [-0.0146],
>         [-0.0130],
>         [-0.0305],
>         [-0.0122],
>         [-1.0000],
>         [-0.0160],
>         [-0.0114],
>         [-0.0215],
>         [-0.0330],
>         [-0.0222],
>         [-0.0310],
>         [-0.0132],
>         [-0.0277]], device='cuda:2') , q1: , q2: , tensor([[0.9944],
>         [0.9995],
>         [0.9966],
>         [1.0000],
>         [0.9944],
>         [0.9980],
>         [0.9992],
>         [1.0000],
>       。。。
>         [0.9787],
>         [0.9964],
>         [0.9900]], device='cuda:2') tensor([[0.9944],
>         [0.9995],
>         。。。
>         [0.9787],
>         [0.9964],
>         [0.9900]], device='cuda:2')  c1_loss(更新order): ， lossq1:  tensor([[0.0000],
>         [0.0711],
>         [0.0000],
>         [0.0983],
>         [0.0000],
>         [0.0000],
>         [0.0462],
>         [0.0983],
>         [0.0000],
>         [0.0000],
>         [0.0000],
>         [0.0000],
>         [0.0987],
>         [0.0000],
>         [0.0775],
>         [0.0000],
>         [0.0000],
>         [0.0000],
>         [0.0000],
>       。。。
>         [0.0955],
>         [0.0932],
>         [0.0000],
>         [0.0000],
>         [0.0000],
>         [0.0000],
>         [0.0000]], device='cuda:2') tensor(1.0849, device='cuda:2', requires_grad=True)





#### 强化学习数据集？



#### 数字化准确率

写好准确率计算函数，写好多个数据集联合训练order的

### actor不可训练！

主要原因是没有从action到奖励的可导的通路，如何给定action，就能得到Q值？

如果不能用c知道A的训练，那么要c有啥用呢？本来R就是准确的了，要C就是为了可到的计算Q。目前可以认为获得了一个attention，那么如何根据attention计算。根据attention计算state基本上就是

强化学习可以是有监督学习吗？？？

使用Q来训练actor达到更好的决策水平，如果已经知道了每次最好的决策，还需要强化学习吗？当然前期可以使用这种方式使actor尽快找到目标。但是后期呢？显然这样可以构成一个非强化学习的解决方案



一个attention，一个state，标记了已选点和候选点，只取候选点，可以构成新的state。新的state可以产生Q值，Q值可以和target拟合

看一下go zero？
